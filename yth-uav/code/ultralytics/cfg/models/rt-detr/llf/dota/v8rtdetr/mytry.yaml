# Ultralytics YOLO 🚀, AGPL-3.0 license
# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect

# Parameters
nc: 80  # number of classes
scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
  # [depth, width, max_channels]
  n: [ 0.33, 0.25, 1024 ]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
  s: [ 0.33, 0.50, 1024 ]  # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
  m: [ 0.67, 0.75, 768 ]   # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
  l: [ 1.00, 1.00, 512 ]   # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
  x: [ 1.00, 1.25, 512 ]   # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs

# YOLOv8.0n backbone
backbone:
  # [from, repeats, module, args]
  - [ -1, 1, Conv, [ 64, 3, 2 ] ]  # 0-P1/2
  - [ -1, 1, Conv, [ 128, 3, 2 ] ]  # 1-P2/4
  - [ -1, 3, C2f, [ 128, True ] ]
  - [ -1, 1, Conv, [ 256, 3, 2 ] ]  # 3-P3/8
  - [ -1, 6, C2f, [ 256, True ] ]
  - [ -1, 1, Conv, [ 512, 3, 2 ] ]  # 5-P4/16
  - [ -1, 6, C2f, [ 512, True ] ]
  - [ -1, 1, Conv, [ 1024, 3, 2 ] ]  # 7-P5/32
  - [ -1, 3, C2f, [ 1024, True ] ]
  - [ -1, 1, SPPF, [ 1024, 5 ] ]  # 9

# YOLOv8.0n head
head:
  - [ -1, 1, Conv, [ 256, 1, 1, None, 1, 1, False ] ]  # 8 input_proj.2
  - [ -1, 1, AIFI, [ 1024, 8 ] ] # 9
  - [ -1, 1, Conv, [ 256, 1, 1 ] ]  # 10, Y5, lateral_convs.0  这个要换个新通道注意力

  - [ -1, 1, CARAFE, [ ] ] # 11
  # ！！ 加入空间注意力结构
  - [ 6, 1, Conv, [ 256, 1, 1, None, 1, 1, False ] ]  # 12 input_proj.1

  # @@
  - [ [ -2, -1 ], 1, Concat, [ 1 ] ] # 13
  - [ -1, 3, RepC3, [ 256, 0.5 ] ]  # 14, fpn_blocks.0
  - [ [ -3,-1 ],1,CWM,[ 256 ] ]  # new15
  - [ -1, 1, Conv, [ 256, 1, 1 ] ]   # 15, Y4, lateral_convs.1 16

  - [ -1, 1, CARAFE, [ ] ] # 16 17
  - [ 5, 1, Conv, [ 256, 1, 1, None, 1, 1, False ] ]  # 17 input_proj.0 18
  # ！！对原始特征层进行通道注意力加持， 然后concat×这个值
  - [ [ -2, -1 ], 1, Concat, [ 1 ] ]  # 18 cat backbone P4 19
  # @@
  # ！！ 在该尺度进行加入 空间注意力
  - [ -1, 3, RepC3, [ 256, 0.5 ] ]    # X3 (19), fpn_blocks.1 20
  # @@
  - [ [ -3,1 ],1,SWM,[ 256 ] ]
  - [ -1, 1, Conv, [ 256, 3, 2 ] ]   # 20, downsample_convs.0 21
  # ！！
  - [ [ -1, 16 ], 1, Concat, [ 1 ] ]  # 21 cat Y4 22
  # @@ 考虑是concat还是repc3×空间注意力的权重 Ortho
  - [ -1, 3, RepC3, [ 256, 0.5 ] ]    # F4 (22), pan_blocks.0 23
  - [ [ 12,-1 ],1,SWM,[ 256 ] ] # 24
  - [ -1, 1, Conv, [ 256, 3, 2 ] ]   # 23, downsample_convs.1
  - [ [ -1, 10 ], 1, Concat, [ 1 ] ]  # 24 cat Y5
  - [ -1, 3, RepC3, [ 256, 0.5 ] ]    # F5 (25), pan_blocks.1
  - [ [ 10,-1 ],1,CWM,[ 256 ] ]

  - [ [ 20, 24, 28 ], 1, RTDETRDecoder, [ nc, 256, 300, 4, 8, 3 ] ]  # Detect(P3, P4, P5)



    #class FSM(nn.Module):
    #    def __init__(self, c1, c2):
    #        super().__init__()
    #        self.conv_atten = nn.Conv2d(c1, c1, 1, bias=False)
    #        self.conv = nn.Conv2d(c1, c2, 1, bias=False)
    #
    #    def forward(self, x: Tensor) -> Tensor:
    #        atten = self.conv_atten(F.avg_pool2d(x, x.shape[2:])).sigmoid()
    #        feat = torch.mul(x, atten)
  #        x = x + feat
  #        return self.conv(x)


